---
categories:
  - blog
date: 2024-11-28 00:00:00 +0900
math: true
title: 勝手に課題に回答してみた
parse_block_html: true
published: true
---

夜に Twitter を眺めていたら課題が目に入ったので勝手に回答してみた。
<https://x.com/sho_yokoi/status/1861714591098798554>

恐らく ASCONE という Autumn School で提示されたものだと思われる。
<http://ascone.brainsci.net/#Lec5>

最初の基礎講義（１時間）を聴いていないので問いのニュアンスを読み誤っているかもしれない。

> 本当は風呂上がりの 1 時間くらいでメモ書きしたことをささっとまとめて 24 時には寝ようと思っていたのだが、Twitter のブックマーク整理を始めてしまい、気づいたら翌 3 時を超えてしまった。

> いざ、ささっとまとめようとしたら、論理の粗さが目立つ。でも、夜も遅いので一旦そのままにして推敲は将来の自分に任せることにする。

## Q1. 内部表現の評価を介して人工ニューラルネットが理解しているかを確認できると言えそうですか？

「タスクが実行できるか」ではなく、その先の「どうやってタスクを実行しているか」という話だと解釈。
「認識している」であれば明らかに成功しているように感じる（顔認証システムとかまさに）。
しかし、「理解している」となると判断が難しい（→ 検討し甲斐がある）。
ここでは「理解している」状態を、データの内挿ではなく外挿ができること、すなわち、学習データに無い/希少なデータに対しても適切な推論ができることだとして考える。

### Q. できるとして、それはなぜですか？

雑な回答としては、先行研究をできる立場から挙げられそう。
<https://openreview.net/forum?id=aajyHYjjsk>

この研究は、言語モデルの内部表現を（probe を通じて？）見ると、入力した文章の Truthfulness が True/False であるかによってしっかりと分かれることを示している。
この研究は、言語モデルはただのパターンマッチング的な推論ではなく、入力文を判断を下しながら読んでいる証拠に見える。

> 正直、動画を見ただけで、論文まで目を通せていないので確証はゼロ。
> <https://youtu.be/x873VHafF40>
>
> もしこの論文が使っている probe が分類タスクのもとで学習しているならこの回答は撤回したくなる。
> True/False が分かれる現象が、学習された言語モデル自身によるものではなく probe が無理やり分けたことが原因に見えるので。

<!-- True な文章と False な文章を見分けられるのは、入力を理解しなくても尤度の大小で判断できるくないか、という反対意見が考えられる。
つまり、内挿しかできないモデルでも学習データにおける入力の内挿具合(?)は測れるので、外挿できなくてもいいのではないか。
この意見を抱く人とは「外挿できない」モデルの認識が自分とは異なると思われる。 -->
<!-- （データの内と外を区別できてる時点で内挿だけしかできないモデルとは一線を画しそう） -->

本命の回答は、氷で作った靴下を裏返せるかという問題を対象に考えたかった。
<https://x.com/imos/status/1644177898319007746>

言語モデルにこういった問題を出して回答を観察することで、理解しているかを観察することはできそう。
しかし、入出力の観察だけだと、「内部表現の評価を介して」いないので課題の回答になっていない。
このネタを内部表現の話に繋げられるのだろうか。
頻度論的に(?)説明するには無理があるので、形式的推論をしているんだろうと考えたくなる。
例えば、モデル内部で `reversible(ice) -> False` みたいな事が起きているかを確認するとか？
裏返せる素材と裏返せない素材が何らかの軸の両極にそれぞれ集まっていて、それをごにょごにょして推論しているとか。

<!-- winogrande の目的はこれに似てる
なんか常識推論の中でも物理っぽくなっちゃうけど、一旦気にしないことにする -->

### Q. できないとして、それはなぜですか？

四則演算程度ならモデル内部の計算過程を追うことができる。
<https://www.anlp.jp/proceedings/annual_meeting/2023/pdf_dir/C4-5.pdf>
しかし、数学オリンピックレベルまで行くと、解けてはいるけど内部メカニズムを解明するのは厳しくなる。
複雑すぎる高度な問題を言語モデルにとって解かれてしまう状況を目にすると「理解している」と感じるが、
複雑すぎるがゆえに内部表現を見ても分からなくなる。

一方で、先程の氷の靴下については、問題自体は複雑ではないので確認できると言えそう。

### Q. この反例を覆せる具体的な評価方法を検討してください

理解していることを確認できない/しにくい問題は、とりあえず理解していないことを確認したくなる。
「理解していることを示せ」ではなく「理解しているかどうかを確認せよ」という問題なら禁じ手ではないはず。

数学オリンピックはゴリゴリのコンタミだろう。
ここから、数学問題なら易しいときは推論によって解決できるけど、難しくなるにつれて暗記したデータをほじくり始めるという仮説が立てられる。
検証方法としては、問題を Easy → Hard の複数段階を用意して、正答するのに推論を要する状況を作る。
例えば、問題文の数字を変えるなど、解法は変わらないけど訓練データとは答えが変わる状況を用意する。
これで仮説通りの結果が出れば、複雑すぎる問題は考えて結論を出しているのではなく統計的にそれっぽく出しているんだろうということになる。

まあまさにそれをしている論文はもうあるのだが。
<https://arxiv.org/abs/2410.05229v1>

余談だが、推論能力を対象にした研究がみんな数学問題を扱っている状況にもやもやしている。
なのでその反動でこれが楽しみ。
<https://nlp-colloquium-jp.github.io/schedule/2024-12-04_terufumi-morishita/>

風呂上がりでは思いつかなかったが、「理解している」かどうかを判断したいなら CoT を用いて推論過程を出させるのは、確認手法の一部として使えそう。

## Q2. そもそも我々人類は人工ニューラルネットや脳について理解することができそうですか？

### Q. ホワイトボックスな人工ニューラルネットにわからないと感じるのはなぜですか？

そもそも、自分は ANN が理解不能な存在だとは思っていない。
何らかの問題が解ける要因は数字の塊のどこかが効いているおかげなのだから、ほじくれば必ず解釈可能ななんかが出てくる。

ただ「わからないと感じる」前提の問題なのでその立場に立って考える。
ぎりぎり分かる例として、解釈性が高いと言われる決定木の複雑バージョンである勾配ブースティング木を挙げる。
まあ、どれだけ複雑になっても所詮条件分岐を繰り返した木なので、分かる範囲内だろう。

ANN が決定木と本質的に異なるのは、分類基準が連続的であること。
決定木はデータを分割しているだけなのに対して、ニューラルネットはデータに潜んだパターンを多次元空間で無理矢理(?)多様体として表現しようとする。
人は非線形性を理解できないので（個人的意見）、ここで分からなくなる。

### Q. 要素還元、線形性、同型性などは世界の本質に迫る道具ですか？人間の認識能力が作った壁ですか？シンプルだけれど今の科学では理解できない例を作ってください

人は接空間やテイラー展開して 2 次以降の項を無視する近似を活用している。
これは人が非線形性を認識できないからだと僕は思っている。（微分幾何は履修していないので接空間は言ってみただけ）
データを局所的に見るときは近似精度がいいので上手くいくが、全体を俯瞰することは線形にしか認識できない人にはできない。
でも、別に埋め込み空間全体を一度に見ることで本質に迫るわけでもなさそうだから、壁ではないかもしれない（りんごとバナナの位置関係は気になっても、りんごと車だったら気にならない）。

<!-- そんな人が生み出すデータは非線形に決まっとる
自然界は非線形現象があるがあれは神が作ったからしゃーない -->

<!-- 微分方程式はきれいなんだけど一般解は出せないみたいな
でもこれ、作ってないな -->

シンプルだけれど今の科学では理解できない例をを考える。（問いの意図を汲み取れた自信はない）

ボトルネック層のニューロン数が n である autoencoder を用意する。
この autoencoder から得られたデータを表現するのに必要最小限のパラメータ数（固有次元）は n になるはず。
しかし、得られたデータから n を逆算するのは簡単にはいかない。
これは、autoencoder がもつ多くの非線形な活性化関数に起因する。

非線形な表現を生成できはするんだけど、非線形な表現を分析することはやりにくいという非対称な状況は気になるところ。
しかし、みんなは ANN にこの逆をやらせている。なので ANN を理解できないのかもしれない。

## Q3. 人工ニューラルネットの理解を通してヒトの理解に近づけそうですか？

### Q.人エニューラルネットとヒトに関して何が一致していることを観察できたら、人エニューラルネットをヒトを理解するモデルとして使えそうですか？

もしヒトが既に ANN を完全理解していたら、ANN とヒトの入出力が一致したらヒトを理解したと言っていいだろう。
（「ヒトってなにしてるの」「ANN みたいなこと」「なるほど」で終わる）。
しかし、ヒトはまだ ANN を完全理解したとは言えないので、こうはならない。

では、すでに理解できる対数線形モデルとヒトの入出力が一致したら、ヒトは対数線形モデルと全く同じ処理をしてるんだと納得できるか？
脳科学の研究者どう感じるのかは分からないが、僕は納得できない。
同じ入出力を再現するモデルは無数にあるはずなので、入出力が一致するだけでは証拠としては弱すぎると感じる。

本気でやるなら ANN の中間ニューロンレベルまでヒトが発する信号と完全一致していてほしい。
でも、ここまで条件を厳しくするとヒトと比較するのは難しくなってくる。
妥協策として一旦ヒトを諦めて脳オルガノイドに電極をつけまくって、ANN で脳オルガノイドが理解できるところから始めるのはどうか（投げやり）。
脳オルガノイドが脳ないしヒトの思考メカニズムと物理・化学的に同じであるならヒトを理解する間接的証拠を出せるかもしれない。

脳オルガノイドを介して真にヒトを理解するためには、脳オルガノイドが意識や感情に芽生える必要があり、そんなことをしたら倫理的にアウトなのだが。

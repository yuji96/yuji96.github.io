---
categories:
  - blog
date: 2025-03-16 00:00:00 +0900
math: true
tags:
  - conf
  - NLP
  - WIP
title: NLP2025 参加記
parse_block_html: true
published: true
---

> 家に帰るまでが遠足であるように、参加記を書くまでが学会である。

[NLP2024 参加記](/posts/NLP2024) が好評 (N=1) だったようなので今年も頑張って書く。

## 今年のひとこと

- 頑張って予定を立てても全ては崩れる
- 発表はメモをしながら聞こう
  - Slack を盛り上げるのはプロに任せてお前（山本）は集中しろ

## 頑張って予定を立てても全ては崩れる

会期中の予定を組むために、去年に引き続き [アブスト一覧ページ](https://yuji96.github.io/miniconf/schedule) を作った。
今回はカレンダーページを追加してセッション毎に発表をフィルタリングできるようにした（去年は [miniconf の本家](https://github.com/Mini-Conf/Mini-Conf) をベースにしたが、今年は [ACL2023](https://github.com/acl-org/acl-2023-miniconf) をベースにしたおかげ）。

これを使ってこんな感じの予定が出来上がった。

<img width="500px" src="/assets/img/posts/NLP2025/nlp2025-schedule.png" alt="Alt text">

実際は朝イチのセッションは寝坊するし（← かす）、午後イチのセッションはお昼ごはん後の懇親が盛り上がって間に合わない（← これは良い）。
ポスターではなかなか自分が発言するターンにならず想定以上に時間が経ってしまうことも少なくない。

なので、来年は何も予定立てずに会場に行って、その場でどの部屋に行くか決めようかなと考えている。
そのための布石：<https://x.com/Mt_B00Ks/status/1900942239054123384>

## 発表はメモをしながら聞こう

今年は人の話をちゃんと聴けなかったというのが一番の反省点だった。
口頭発表の時間は短いもののその内容はとても高密度なので、聴講中に浮かんだ疑問について思い巡らしていると面白い話を聞き逃してしまうなんてことが多々あった。
帰りの空港行きのバスで考えた解決策は、

- 聴く時間と考える時間は分ける
- 発表中に興味を引かれた点を後で思い出せるようにメモを残す
- それ以外ではスマホや PC は触らない（当たり前）
- これらのことがちゃんとできるまで Slack の盛り上げは諦める

という当たり前のことである。
普段の研究ゼミや会議中の議事録などを適当にやっていたツケが回ってきたのだろう。

## 発表・予稿の感想

上で言い訳したように、今年は濃い感想を書けない気がする。
また、記憶違いで頓珍漢なことをいっているかもしれないので注意。

### D1-4 ベイズ教師なし文境界認識

内海 慶 (SB Intuitions), ○ 持橋 大地 (統数研/国語研)

句点で区切るというヒューリスティックではなく LLM でもない文分割手法。
最近、最後に句点があると怖そうな印象を与えるというネットニュースがあったが、
Twitter のような崩れた文章ではヒューリスティックは使いにくい。
しかし、文分割という問題に LLM を持ち出すのはオーバーな気もする。

この発表の提案手法を使うと Twitter のとても崩れた文章でも妥当な境界で分割できていた。
セミマルコフモデル＋動的計画法＋ MCMC など、名前は聞いたことあるがよく理解はしていない手法が使われている。問題設定が文分割という簡単なのでこれらの手法の入門としても良さそう。

分岐エントロピーを使うと形態素境界を検出できるというハリスの文節原理というものがあるが、
持橋先生曰くこれは理由が分からないけどなんかそれっぽく分割できる存在とのこと。
分岐エントロピーの方が直感的な立式ではあるが明示的にモデリングしているわけではないので、
文分割のための指標というのは違うという気持ちには強く同意した。

### P4-21 言語モデルのふるまいと多重実現

○ 坪井 祥吾 (一橋大), 菅原 朔 (NII)

人間と言語モデルがとあるタスクを同程度に実行できたとしても、
両者の推論過程が同じ方法で実現されているとは言えない。
では、同じというためにはどのような壁を乗り越える必要があるのかについて
科学哲学の観点から論じている。

モデル解釈界隈では「モデルの中身がこうなっているから人間の認知もそうなんだ」という
論理を見かけることがあるが、僕は多重実現していると（特に理由もなく）強く思っているので、
そのような主張は受け入れ難いと感じている。
この予稿はそのような研究を納得や批判をしたり、
手元の実験結果からどこまでの範囲のことを主張できるかを検討したるするときの指針になりうる。
言語モデルの理解が進み、その知見から何かが分かりそうと思いたくなる現在において
とても重要な議論だと感じた。

### P4-24 言語研究における科学的理解と言語モデル

○ 鈴木 陽登 (慶應大), 菅原 朔 (NII)

取り上げた理由は P4-21 とほぼ同様。

「〜を解明した」というのはどういう状況なのかについて科学哲学の観点から論じている。
モデル解釈系の目標はモデルを理解ることなので内省する必要があると感じた。

### Q4-21 情報圧縮を用いた訓練データの重複削減

○ 堤田 恭太, 村瀬 文彦, 三谷 陽 (デンソー)

あるサンプルが訓練データに含まれいるかどうかを提案スコアによって測ることで
学習データの重複削減を行う。
提案スコアは情報圧縮距離 NCD にインスパイアされている：

<!-- $$
\text{Score}(\mathscr{T}, c_i)
= \dfrac{C(\mathscr{T} c_i) - \max(C(\mathscr{T}), C(c_i))}
{\min(C(\mathscr{T}), C(c_i))}
$$ -->

$$
\text{Score}(\mathscr{T}, c_i)
= \dfrac{C(\mathscr{T} c_i) - C(\mathscr{T})}{C(c_i)}.
$$

ただし、$C$ はテキストを gzip 圧縮したときのデータサイズ、
$\mathscr{T}, c_i$はそれぞれ訓練データ全体と追加データ候補。

分子はサンプル $c_i$ によって情報がどれだけ追加されるかを意味していて、
サンプル $c_i$ が既に訓練データ $\mathscr{T}$ に含まれていたらほぼゼロになる。
逆に新規のサンプルであれば $C(\mathscr{T} c_i)$ のサイズは大体 $C(c_i)$ のサイズだけ増えることになるので、正規化後の Score は 1 となる。

gzip 圧縮後のサイズを用いた指標というのが興味深かった。
gzip ということは本質的には ngram レベルの表層的類似度に依存してそうなので、
BLEU でも代替できるのかが気になる。
また $C(\mathscr{T} c_i)$ を何度も計算するのが重そう（効率的な方法があるのかな）。
実験結果では、ナイーブな完全一致サンプル削除よりもランダムサンプル削除のほうが高性能だったのが謎だった。高頻度のデータは実用上もたくさん出てくるから過剰適合してしまったほうがベンチマーク上はスコアが高くなるみたいなことなのだろうか。

### B4-6 作業記憶の発達的特性が言語獲得の臨界期を形成する

○ 三田 雅人 (東大/サイバーエージェント), 吉田 遼, 深津 聡世, 大関 洋平 (東大)

LLM と人間が学習にかけているリソースのギャップには自分も興味がある話なので
後ほど精読したい。

認知科学の知見が NLP に活かされるというのは気持ちが良い話（人間の知見を活かさずにサイボーグ化していく道もそれはそれでありなのだが）。
個人的には「言語モデルではこうだから人間の認知もこう」という流れはモヤッとするのだが、この発表のように「人間の認知がこうだという仮説が言語モデル上で実証されたので仮説を指示する証拠となる」という流れには納得感があった。

ALiBi を使うとサプライザルの推定精度が向上するという引用先の話も気になる

> [Linear Recency Bias During Training Improves Transformers’ Fit to Reading Times (Clark et al., COLING 2025)](https://aclanthology.org/2025.coling-main.517/)

### P5-19 音声トークナイズが音声言語モデルの性能に与える影響の調査

○ 神藤 駿介 (東大), 宮尾 祐介 (東大/NII), 高道 慎之介 (慶應大/東大)

（ここ発表に対する感想というより、音声トーカナイズという概念に対する感想になってしまっている）

音声トーカナイズという言葉を最近よく聞くので気になっていた。
自分のイメージは、これまでは speech →(ASR)→ text →(LM)→ pred. という古典的なパイプライン処理を speech →(tokenize)→(LM)→ pred. という単純なものに移行するためのキーとなっているのが音声トーカナイズということなのだが合っているのだろうか。

予稿を読んでみたら、音声トーカナイズとは speech →(HuBERT)→ emb array →(k-means)→IDs という手順らしい。HuBERT は自己教師あり学習（確か MLM みたいなやつ）をしているので、音声トーカナイズの過程は音声のみで構築された textless と言える。

HuBERT の埋め込みをそのまま使わずに ID 化するのってなんでなんだろ。
また、もともと言語モデルとして学習された OPT が音声トークン列で 1 epoch 学習すると音声言語モデルになるの、エグない？何が起きてるんだろ。

### A6-1 スパースオートエンコーダーを用いた大規模言語モデルのチェックポイント横断分析

○ 稲葉 達郎 (京大/NII), 乾 健太郎 (MBZUAI/東北大/理研), 宮尾 祐介 (東大/NII), 大関 洋平 (東大), Benjamin Heinzerling (理研/東北大), 高木 優 (NII)

本人にも伝わってしまったことだし本音で書くと、
稲葉くんは YANS2023 での金髪のイメージが強かったり PFN で 🙇‍♂️ をしていたりぶっちゃけ世界観が違うように感じていたが、今回の口頭発表では黒髪ですごく優しい口調だったのでイメージが好転した。同じホテルだった。

まず、Sparse Autoencoder の説明がめちゃくちゃ分かりやすい。NLP2025 の one of the best slides。
本題では、埋め込み表現分析では学習済みモデルが対象とされがちなのに対して、この発表では学習中を対象としている。
結果は、序盤ではあまり特徴が見られなかったのが、中盤ではトークンレベルのパターンが見られて、終盤では概念レベルのパターンに汎化(?)されていくというもの。
Sparse Autoencoder で言語モデルを解釈しようという話はよく見るが、結果が綺麗すぎて「ほんまかよ」とよく思ってしまうのだが、そろそろ真面目に向き合わねばという気持ちになった（自戒：グチグチ言う前に手元で動かしなさい）。

中盤でトークンレベルのパターンが見られるとのことだったが、このときの SAE：

$$
z=\text{TopK}(W_{\text{enc}}(x-b_{\text{pre}}))
$$

の $W_{\text{enc}}\in\mathbb{R}^{n\times d}$ って語彙サイズ $n$ の埋め込み行列とほぼ同じになってそう。
だとしても「中間層でトークンが見分けられるようになっている」以上に興味深い解釈はできなそうなのでこれはただの余談。

### A6-5 Derivational Probing：言語モデルにおける統語構造構築の解明

○ 染谷 大河, 吉田 遼, 谷中 瞳, 大関 洋平 (東大)

WIP

### A7-3 構成的汎化における Transformer の内部機序の分析

○ 九門 涼真, 谷中 瞳 (東大)

WIP

博士課程かと思ってました

### Q8-15 Sparse Autoencoders as a Tool for Steering the Output Language of Large Language Models

○◊Sebastian Zwirner, Wentao Hu, 青木 洸士郎, 河原 大輔 (早大)

WIP

### E9-3 二重課題は言語モデルの合理的な言語理解ストラテジーを促進する

○ 江村 玲 (東北大/NII), 菅原 朔 (NII)

WIP

個人的に best presentation だった。

### Q9-21 文の埋め込みに効果的な静的単語ベクトルの獲得

○ 和田 崇史, 平川 優伎, 清水 良太郎, 川島 貴大, 斎藤 侑輝 (ZOZO NEXT)

WIP

### Q9-2 言語のインクリメンタルな処理の仕組みは普遍的か？：投機性による parsing strategy 再考

○ 石井 太河, 宮尾 祐介 (東大)

WIP

ポスターの作りがシンプルなのだが RQ → Ans → Support という構成が上から下に流れていくのがわかりやすかった

## 懇親

記憶の中にある印象深いフレーズ

- 最後は腕力
- 「AI って何やってるか分からないから仕事任せたくない」って言われたら「でも脳の構造分からないのに人には仕事任せるんですね」って返してる
- 研究の方向性を決めるとき、面白さよりも怒りのほうが照準を絞りやすい
  - 自分の場合は「AI はブラックボックスという発言」と「別に大した事ない人間と同レベルになるまでに LLM がかけているコスト」が怒りの対象かもしれない

あと、D の会懇親会は D という名前に威圧感があるかもしれないけど、
ガチ研究トークは置いといてずっと世間話してました。
自分には微塵も関係ない結婚トークが始まったときは居た堪れなくなりましたが。

<!-- D5-4 RNN の回帰行列を凍結しても統語構造の獲得は損なわれない
P2-7 大規模言語モデル内部における言語と計算領域の区分
P2-19 多言語モデルには言語非依存の処理系統が存在するか -->
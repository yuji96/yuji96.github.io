---
categories:
  - blog
date: 2023-12-15 00:00:00 +0900
math: true
tags:
  - math
title: PCA ã¯ PCoA ã®ç‰¹æ®Šã‚±ãƒ¼ã‚¹ã‚‰ã—ã„
parse_block_html: true
published: true
---

ä»Šé€±ã€PCoA ã¨ã„ã†æ‰‹æ³•ã‚’çŸ¥ã‚Šã€ã¡ã‚‡ã“ã¡ã‚‡ã“èª¿ã¹ã¦ã„ã‚‹ã¨ã€Œéé¡ä¼¼åº¦ã‚’ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã¨ã™ã‚‹ã¨ã€PCoA ã¨ PCA ã¯ç­‰ä¾¡ã«ãªã‚‹ã€ã¨ã„ã†è¨˜è¿°ã‚’ãŸãã•ã‚“è¦‹ã¾ã—ãŸã€‚
ã—ã‹ã—ã€ãã®è¨¼æ˜ã‚’è¦‹ã¤ã‘ã‚‰ã‚Œãªã‹ã£ãŸã®ã§è‡ªåˆ†ã§ã‚„ã‚Šã¾ã—ãŸã€‚

## PCoA ã¨ã¯

ç•¥ã•ãšã«æ›¸ãã¨ Principal Coordinate Analysis (ä¸»åº§æ¨™åˆ†æ) ã§ã™ã€‚
[MDS (Multidimensional scaling, å¤šæ¬¡å…ƒå°ºåº¦æ§‹æˆæ³•)](https://en.wikipedia.org/wiki/Multidimensional_scaling) ã¨ã„ã†æ‰‹æ³•ã®ã†ã¡ã®ä¸€ã¤ã‚‰ã—ã„ã§ã™ã€‚

> "ã€‡ã€‡" ã£ã¦ã„ã†æ‰‹æ³•è¦‹ã¤ã‘ãŸã‚‰ "ä¸€èˆ¬åŒ–ã€‡ã€‡" ã£ã¦ã„ã†æ‰‹æ³•ã‚ã‚ŠãŒã¡

æ¦‚è¦ã‚’è€ƒãˆã‚‹ã®ãŒé¢å€’ãªã®ã§ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‹ã‚‰èª¬æ˜ã—ã¡ã‚ƒã„ã¾ã™ã€‚
è¨˜å·ã¯ [wiki](https://en.wikipedia.org/wiki/Multidimensional_scaling#Classical_multidimensional_scaling) ã¨åŒã˜ã§ã™ã€‚

### 1. éé¡ä¼¼åº¦è¡Œåˆ— (proximity matrix) ã‚’å®šç¾©ã™ã‚‹ã€‚

éé¡ä¼¼åº¦è¡Œåˆ—ã‚’ $D$ ã¨ã—ãŸã¨ãã€ãã® $(i,j)$ æˆåˆ† $d_{ij}$ ã¯ã€$i$ ç•ªç›®ã®è¦³æ¸¬ $x_i$ ã¨ $j$ ç•ªç›®ã®è¦³æ¸¬ $x_j$ ã®éé¡ä¼¼åº¦ã§ã™ã€‚å…·ä½“ä¾‹ã‚’æŒ™ã’ã‚‹ãªã‚‰ã“ã‚“ãªæ„Ÿã˜ã€‚

|     |  ğŸ   | ğŸ  |  ğŸ§   | ğŸ¨  |  ğŸ¶   | ğŸº  |
| :-: | :---: | :-: | :---: | :-: | :---: | :-: |
| ğŸ  |   0   |  -  |   -   |  -  |   -   |  -  |
| ğŸ  | _0.1_ |  0  |   -   |  -  |   -   |  -  |
| ğŸ§  |  0.5  | 0.6 |   0   |  -  |   -   |  -  |
| ğŸ¨  |  0.6  | 0.6 | _0.2_ |  0  |   -   |  -  |
| ğŸ¶  |  0.8  | 0.7 |  0.8  | 0.9 |   0   |  -  |
| ğŸº  |  0.9  |  1  |   1   | 0.8 | _0.2_ |  0  |

éé¡ä¼¼åº¦ã®æŒ‡æ¨™ã¯ä»»æ„ã§ã™ã€‚ãŸã ã—ã€è‡ªåˆ†è‡ªèº«ã¨ã®éé¡ä¼¼åº¦ã¯ 0 ã§ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã€‚

ã“ã®å¾Œã®è¨ˆç®—ã«ç”¨ã„ã‚‹ã®ã¯ã€è¡Œåˆ— $D$ ã®å„è¦ç´ ã‚’ 2 ä¹—ã—ãŸå¹³æ–¹éé¡ä¼¼åº¦è¡Œåˆ— (squared proximity matrix) $D^{(2)}$ ã§ã™ã€‚

$$
(D^{(2)})_{ij} = d_{ij}^2
$$

### 2. äºŒé‡ä¸­å¿ƒåŒ– (double centering)

ã“ã®å‡¦ç†ã¯åˆã‚ã¦çŸ¥ã‚Šã¾ã—ãŸã€‚[ä¸­å¿ƒåŒ–è¡Œåˆ— (centering matrix
)](https://en.wikipedia.org/wiki/Centering_matrix) $C\in\mathbb{R}^{n\times n}$ ã‚’æ¬¡ã®ã‚ˆã†ã«å®šç¾©ã—ã¾ã™:

$$
C = I - \frac{1}{n}\mathbf{1}\mathbf{1}^T
$$

ã“ã‚Œã‚’è¡Œåˆ— $D^{(2)}$ ã®ä¸¡ã‚µã‚¤ãƒ‰ã‹ã‚‰æ›ã‘ã¦:

$$
B = -\frac{1}{2} C D^{(2)} C
$$

ã¨ã—ã¾ã™ã€‚ã™ã‚‹ã¨è¡Œåˆ— $B$ ã®å„è¡Œå’Œãƒ»åˆ—å’ŒãŒã‚¼ãƒ­ã«ãªã‚‹ã‚‰ã—ã„ã§ã™ [[å‚è€ƒ](https://www.hello-statisticians.com/explain-terms-cat/double_centering1.html)]ã€‚

### 3. å›ºæœ‰å€¤åˆ†è§£ã—ã¦è»¸ã‚’æ±‚ã‚ã‚‹

å…ˆã»ã©ã®è¡Œåˆ— $B$ ã‚’å›ºæœ‰å€¤åˆ†è§£ã—ã¾ã™:

$$
B = V\Lambda V^T.
$$

ã“ã®å³è¾ºã®åŠåˆ† (?) ã‚’è¨ˆç®—ã™ã‚‹ã¨å¤‰æ›å¾Œã®è¡¨ç¾:

$$
Y = V\Lambda^{1/2}
$$

ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

> å›ºæœ‰å€¤åˆ†è§£ãã‚“ã€ç·šå½¢åŸºåº•å¤‰æ›ç³»ã®æ–‡è„ˆã§ã¯ã©ã“ã«ã§ã‚‚ã„ã¾ã™ã­ã€‚ä»²è‰¯ããªã‚ŠãŸã„ã€‚

## (å€‹äººçš„) PCA ã¨ PCoA ã®é•ã„

PCA ã¯ãƒ‡ãƒ¼ã‚¿ $X$ ã‹ã‚‰å…±åˆ†æ•£è¡Œåˆ— $X^TX$ ã‚’ã¤ãã£ã¦ã€ã‚³ãƒ¬ã‚’å›ºæœ‰å€¤åˆ†è§£ã—ã¾ã™ã€‚ã—ã‹ã—ã€PCoA ã¯éé¡ä¼¼åº¦è¡Œåˆ— $D$ ãŒã‚¹ã‚¿ãƒ¼ãƒˆåœ°ç‚¹ãªã®ã§ã€éé¡ä¼¼åº¦ã¨ã—ã¦ã„ã‚ã‚“ãªæŒ‡æ¨™ã‚’è€ƒãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

éé¡ä¼¼åº¦ dissim($x_i$, $x_j$) ãŒå®šç¾©ã§ãã‚Œã°ã„ã„ã¨ã„ã†ã“ã¨ã¯ã€è¦³æ¸¬ $x_i$ ãŒãƒ™ã‚¯ãƒˆãƒ«ã§ã‚ã‚‹å¿…è¦ã‚‚ãªã„ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚PCoA ã¯æ¬¡å…ƒåœ§ç¸®ã¨ã—ã¦ã‚‚ä½¿ã‚ã‚Œã‚‹ãŒã€ãã‚‚ãã‚‚æ•°ç©ºé–“ä¸Šã«ãªã‹ã£ãŸã‚‚ã®ã‚’å¯è¦–åŒ–ã™ã‚‹ã¨ã„ã†è§£é‡ˆã‚‚ã§ãã‚‹ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚

## æœ¬é¡Œ: $d_{ij} = ||x_i - x_j||$ ãªã‚‰ PCoA ã¯ PCA

ã¨ã‚Šã‚ãˆãšã€è¡Œåˆ— $B=-\frac{1}{2}CD^{(2)}C$ ã® $(i,j)$ æˆåˆ†ã‚’æ°—åˆã„ã§æ±‚ã‚ã¾ã™ã€‚ç°¡ç•¥åŒ–ã®ãŸã‚ã«ä»¥ä¸‹ã®å¼ã‚’å®šç¾©ã—ã¦ãŠãã¾ã™ã€‚

$$
d_{\bullet j}^2 := \frac{1}{n}\sum_i d_{ij}^2, \quad
d_{i \bullet}^2 := \frac{1}{n}\sum_j d_{ij}^2, \quad
d_{\bullet \bullet}^2 := \frac{1}{n^2}\sum_{i,j} d_{ij}^2
$$

ã¾ãšã€å·¦å´ï¼’ã¤ã®è¡Œåˆ—ã®ç©ã€‚

$$
(CD^{(2)})_{ij}
= \sum_k c_{ik} d_{kj}^2
= \sum_k \left(\delta_{ik} - \frac{1}{n}\right) d_{kj}^2
= d_{ij}^2 - d_{\bullet j}^2
$$

> ã€ŒçŸ¥ã£ã¨ã‚‹ã‚ã€ã¨è¨€ã‚ã‚Œã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã‘ã©ã€$\delta_{ij}$ ã¯ç·å’Œ $\sum$ ã‚’æ¶ˆã—ã¦ãã‚Œã‚‹ã®ãŒãƒã‚¤ãƒ³ãƒˆã€‚

æ¬¡ã«å…¨ä½“ã€‚

$$
\begin{aligned}
(CD^{(2)}C)_{ij}
&= \sum_k (d_{ik}^2 - d_{\bullet k}^2) \left( \delta_{kj} - \frac{1}{n} \right) \\
&= d_{ij}^2 - d_{\bullet j}^2 - d_{i\bullet}^2 + d_{\bullet\bullet}^2
\end{aligned}
$$

> ãªã‚“ã‹ã“ã†ã„ã†å¼ã€å®Ÿé¨“è¨ˆç”»æ³•ã¨ã‹ã§ã‚‚è¦‹ãŸã“ã¨ã‚ã‚‹æ°—ãŒã™ã‚‹

ã“ã“ã§ã€å®Ÿéš›ã« $d_{ij} = \|\| x_i - x_j \|\|$ ã‚’ä»£å…¥ã—ã¦ã„ãã€‚ãŸã ã—ã€$x$ ã¯ä¸­å¿ƒåŒ–ã•ã‚Œã¦ã„ã‚‹ã¨ã™ã‚‹ ( $\sum_i x_i = 0$ )ã€‚

1 é …ç›® $d_{ij}^2$:

$$
d_{ij}^2 = \| x_i - x_j \|^2 = \|x_i\|^2 + \|x_j\|^2 - 2x_i^Tx_j
$$

2 é …ç›® $d_{\bullet j}^2$:

$$
d_{\bullet j}^2
= \frac{1}{n}\sum_i \left( \|x_i\|^2 + \|x_j\|^2 - 2x_i^Tx_j \right)
= \overline{\|x\|} + \|x_j\|^2 + 0
$$

ãŸã ã—ã€$\overline{\|\|x\|\|} := \frac{1}{n}\sum_i \|\|x_i\|\|$.

3 é …ç›® $d_{i \bullet}^2$:

$$
d_{i \bullet}^2
= \frac{1}{n}\sum_j \left( \|x_i\|^2 + \|x_j\|^2 - 2x_i^Tx_j \right)
= \|x_i\|^2 + \overline{\|x\|} + 0
$$

4 é …ç›® $d_{\bullet \bullet}^2$:

$$
d_{i \bullet}^2
= \frac{1}{n^2}\sum_{i,j} \left( \|x_i\|^2 + \|x_j\|^2 - 2x_i^Tx_j \right)
= 2\overline{\|x\|} + 0
$$

å…¨éƒ¨åˆã‚ã›ã‚‹ã¨:

$$
\begin{aligned}
  (CD^{(2)}C)_{ij}
  &= (\|x_i\|^2 + \|x_j\|^2 - 2x_i^Tx_j) \\
  &- (\overline{\|x\|} + \|x_j\|^2) \\
  &- (\|x_i\|^2 + \overline{\|x\|}) \\
  &+ 2\overline{\|x\|} \\
  &= - 2x_i^Tx_j
\end{aligned}
$$

$$
\therefore (B)_{ij} = -\frac{1}{2} (CD^{(2)}C)_{ij} = x_i^Tx_j
$$

$$
\therefore B = X^TX
$$

ã‚ã‚Œã€å…±åˆ†æ•£è¡Œåˆ—ã«ãªã£ã¡ã‚ƒã£ãŸã€‚ã“ã®ã‚ã¨ PCoA ã§ã¯ã€è¡Œåˆ— $B$ ã‚’å›ºæœ‰å€¤åˆ†è§£ã—ã¾ã™ã€‚ãªã®ã§ã€å…±åˆ†æ•£è¡Œåˆ—ã«å¯¾ã—ã¦å›ºæœ‰å€¤åˆ†è§£ã‚’è¡Œã† PCA ã¨ç­‰ä¾¡ã«ãªã‚Šã¾ã™ã€‚

## å®Ÿè£…ã§ç¢ºèª

å›ºæœ‰å€¤åˆ†è§£ã‚’ç”¨ã„ãŸ PCoA ã¯ [scikit-bio](https://scikit.bio/docs/latest/generated/skbio.stats.ordination.pcoa.html#skbio.stats.ordination.pcoa) ã§åˆ©ç”¨ã§ãã¾ã™ã€‚

è©¦ã—ã«å˜èªåŸ‹ã‚è¾¼ã¿ã«å¯¾ã—ã¦ PCA ã¨ PCoA ã®ä¸¡æ–¹ã‚’é©ç”¨ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

```python
import plt
from transformers import AutoModel
from skbio.stats.ordination import pcoa
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import euclidean_distances

model = AutoModel.from_pretrained("bert-base-cased")
we = model.embeddings.word_embeddings.weight.data[1000:4000]

x, y = PCA(2).fit_transform(we).T
plt.scatter(x, y, label="PCA")

x, y = pcoa(euclidean_distances(we)).samples.values.T[:2]
plt.scatter(-x, y, c="r", marker="x", label="PCoA")

plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
```

<img src="/assets/img/posts/PCA-vs-PCoA.png">

ã‚ãƒ¼ã€ä¸€è‡´ã—ãŸã€‚
